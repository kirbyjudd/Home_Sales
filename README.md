# Home_Sales
Module 22 Challenge

## Background
In this challenge, you'll use your knowledge of SparkSQL to determine key metrics about home sales data. Then you'll use Spark to create temporary views, partition the data, cache and unccache a temporary table, and verify that the table has been uncached.

## Summary
For this challenge I had to import the initial Home_sales_colab starter code into Google Colab. From there I was tasked to answer several questions with SQL queries and temporary tables. I also put into practice caching and uncaching as well as fromatting into parquet data.

The analysis questions included:
- What is the average price for a four-bedroom house sold for each year? Round off your answer to two decimal places.
- What is the average price of a home for each year it was built that has three bedrooms and three bathrooms? Round off your answer to two decimal places.
- What is the average price of a home for each year that has three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet? Round off your answer to two decimal places.
- What is the "view" rating for homes costing more than or equal to $350,000? Determine the run time for this query, and round off your answer to two decimal places.

## File Location
The main ipnyb is located in main and is called "Home_Sales_colab.ipynb". 
